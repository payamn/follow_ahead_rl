# Environment parameters

env: gazebo-v0
state_dim: 26
action_dim: 2
action_low: -1
action_high: 1
num_agents: 4
random_seed: 2019
policy_weights: /home/payam/ros2_ws/src/follow_ahead_rl/scripts/results/
policy_weights_best: /home/payam/ros2_ws/src/follow_ahead_rl/scripts/results/policy_best2.pt
value_weights: /home/payam/ros2_ws/src/follow_ahead_rl/scripts/results/
value_weights_best: /home/payam/ros2_ws/src/follow_ahead_rl/scripts/results/value_best2.pt

# Training parameters

model: d4pg
batch_size: 64
num_steps_train: 9000000 # number of episodes from all agents
max_ep_length: 35 # maximum number of steps per episode
replay_mem_size: 200000 # maximum capacity of replay memory
priority_alpha: 0.5 # controls the randomness vs prioritisation of the prioritised sampling (0.0 = Uniform sampling, 1.0 = Greedy prioritisation)
priority_beta_start: 0.35 # starting value of beta - controls to what degree IS weights influence the gradient updates to correct for the bias introduces by priority sampling (0 - no correction, 1 - full correction)
priority_beta_end: 0.95 # beta will be linearly annelaed from its start value to this value thoughout training
discount_rate: 0.95 # Discount rate (gamma) for future rewards
n_step_returns: 5 # number of future steps to collect experiences for N-step returns
update_agent_ep: 1 # agent gets latest parameters from learner every update_agent_ep episodes
replay_queue_size: 4000 # queue with replays from all the agents
batch_queue_size: 1000 # queue with batches given to learner
replay_memory_prioritized: 0
num_episode_save: 1000
device: cuda


# Network parameters

critic_learning_rate: 0.002
actor_learning_rate: 0.002
dense_size: 512 # size of the 2 hidden layers in networks
final_layer_init: 0.004
num_atoms: 128 # number of atoms in output layer of distributed critic
v_min: -40.0 # lower bound of critic value output distribution
v_max: 0.0 # upper bound of critic value output distribution
tau: 0.002 # parameter for soft target network updates

# Miscellaneous
results_path: results

